trainer: imgrec_trainer

train_dataset:
  name: imgrec_dataset
  args:
    imageset:
      name: celeba
      args: {root_path: $load_root$/celeba, split: train}
    resize: 178
  loader:
    batch_size: 12
    num_workers: 16

test_dataset:
  name: imgrec_dataset
  args:
    imageset:
      name: celeba
      args: {root_path: $load_root$/celeba, split: test}
    resize: 178
  loader:
    batch_size: 12
    num_workers: 8

model:
  name: trans_inr
  args:
    tokenizer:
      name: imgrec_tokenizer
      args: {input_size: 178, patch_size: 18, padding: 1}
    # hyponet:
    #   name: hypo_mlp
    #   args: {in_dim: 2, out_dim: 3, out_bias: 0.5, depth: 5, hidden_dim: 256, use_pe: true, pe_dim: 128}
    hyponet:
      name: hypo_shacira
      args: {}
    n_groups: 64
    # transformer_encoder:
    #   name: transformer_encoder
    #   args: {dim: 768, depth: 6, n_head: 12, head_dim: 64, ff_dim: 3072}
    transformer_encoder:
      name: transformer_encoder
      args: {dim: 768, depth: 2, n_head: 12, head_dim: 64, ff_dim: 3072}

optimizer:
  name: adam
  args: {lr: 1.e-2, weight_decay: 1.e-5}
max_epoch: 80

eval_epoch: 1
vis_epoch: 1
